<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="知识蒸馏"><meta name="keywords" content="计算机视觉,知识蒸馏"><meta name="author" content="re-burn"><meta name="copyright" content="re-burn"><title>知识蒸馏 | re-burn 笔记堆</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5"><span class="toc-number">1.</span> <span class="toc-text">论文链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">2.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E3%80%8C%E8%92%B8%E9%A6%8F%E3%80%8D%EF%BC%9F"><span class="toc-number">3.</span> <span class="toc-text">为什么叫「蒸馏」？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E%E7%9B%B4%E6%8E%A5%E4%BC%98%E5%8C%96logits%E5%B7%AE%E5%BC%82%E7%9B%B8%E6%AF%94"><span class="toc-number">4.</span> <span class="toc-text">与直接优化logits差异相比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E8%AE%BA"><span class="toc-number">5.</span> <span class="toc-text">实验与结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A5%E5%85%85"><span class="toc-number">6.</span> <span class="toc-text">补充</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">re-burn</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">8</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">9</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">4</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">re-burn 笔记堆</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">知识蒸馏</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-04-12</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">日常学习笔记</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="论文链接"><a href="#论文链接" class="headerlink" title="论文链接"></a>论文链接</h2><p><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1503.02531.pdf">https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1503.02531.pdf</a></p>
<p>Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>模型压缩：在深度学习的背景下，为了达到更好的预测，常常会有两种方案：1. 使用过参数化的深度神经网络，这类网络学习能力非常强，因此往往加上一定的正则化策略(如dropout)；2. 集成模型(ensemble)，将许多弱的模型集成起来，往往可以实现较好的预测。这两种方案无疑都有较大的「支出」，需要的计算量和计算资源很大，对部署非常不利。这也就是模型压缩的动机：我们希望有一个规模较小的模型，能达到和大模型一样或相当的结果。当然，从头训练一个小模型，从经验上看是很难达到上述效果的，也许我们能先训练一个大而强的模型，然后将其包含的知识转移给小的模型</p>
<p>Rich Caruana等人在[1]中指出，可以让新模型近似(approximate)原模型(模型即函数)。注意到，在机器学习中，我们常常假定输入到输出有一个潜在的函数关系，这个函数是未知的：从头学习一个新模型就是从<strong>有限的数据</strong>中近似一个<strong>未知的函数</strong>。如果让新模型近似原模型，因为原模型的<strong>函数是已知的</strong>，我们可以使用很多非训练集内的伪数据来训练新模型，这显然要更可行。</p>
<p>这样，原来我们需要让新模型的softmax分布与真实标签匹配，现在只需要让新模型与原模型在给定输入下的softmax分布匹配了。直观来看，后者比前者具有这样一个优势：经过训练后的原模型，其softmax分布包含有一定的知识——真实标签只能告诉我们，某个图像样本是一辆宝马，不是一辆垃圾车，也不是一颗萝卜；而经过训练的softmax可能会告诉我们，它最可能是一辆宝马，不大可能是一辆垃圾车，但绝不可能是一颗萝卜</p>
<h2 id="为什么叫「蒸馏」？"><a href="#为什么叫「蒸馏」？" class="headerlink" title="为什么叫「蒸馏」？"></a>为什么叫「蒸馏」？</h2><p>接续前面的讨论，我们的目标是让新模型与原模型的softmax输出的分布充分接近。直接这样做是有问题的：在一般的softmax函数中，自然指数 $$e$$ 先拉大logits之间的差距，然后作归一化，最终得到的分布是一个arg max的近似 ，其输出是一个接近one-hot的向量，其中一个值很大，其他的都很小。这种情况下，前面说到的「可能是垃圾车，但绝不是萝卜」这种知识的体现是非常有限的。相较类似one-hot这样的硬性输出，我们更希望输出更「软」一些。</p>
<p>一种方法是直接比较logits来避免这个问题。具体地，对于每一条数据，记原模型产生的某个logits是 $$v_i$$ ，新模型产生的logits是  $$z_i$$，我们需要最小化</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D(z_i-v_i)%5E2%5Ctag%7B1%7D" alt="[公式]"></p>
<p>文献[2]提出了更通用的一种做法。考虑一个广义的softmax函数</p>
<p><img src="https://www.zhihu.com/equation?tex=q_i=%5Cfrac%7B%5Cexp(z_i/T)%7D%7B%5Csum_j%5Cexp(z_j/T)%7D%5Ctag%7B2%7D" alt="[公式]"></p>
<p>其中$$T$$是温度，这是从统计力学中的玻尔兹曼分布中借用的概念。容易证明，当温度 $$T$$趋向于0时，softmax输出将收敛为一个one-hot向量（证明可以参考我之前的文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79585726">浅谈Softmax函数</a>，将$$\beta$$ 替换为 $$1 / T$$即可)；温度趋$$T$$向于无穷时，softmax的输出则更「软」。因此，在训练新模型的时候，可以使用较高的$$T$$使得softmax产生的分布足够软，这时让新模型（同样温度下）的softmax输出近似原模型；在训练结束以后再使用正常的温度$$T=1$$来预测。具体地，在训练时我们需要最小化两个分布的交叉熵(Cross-entropy)，记新模型利用公式(2)产生的分布是$$q$$，原模型产生的分布是，则我们需要最小化</p>
<p><img src="https://www.zhihu.com/equation?tex=C=-p%5E%5Ctop%5Clog+q%5Ctag%7B3%7D" alt="[公式]"></p>
<p>在化学中，蒸馏是一个有效的分离沸点不同的组分的方法，大致步骤是先升温使低沸点的组分汽化，然后降温冷凝，达到分离出目标物质的目的。在前面提到的这个过程中，<strong>我们先让温度</strong> $$T$$ <strong>升高，然后在测试阶段恢复「低温」，从而将原模型中的知识提取出来，因此将其称为是蒸馏</strong>，实在是妙。</p>
<p>当然，如果转移时使用的是有标签的数据，那么也可以将标签与新模型softmax分布的交叉熵加入到损失函数中去。这里需要将式(3) 乘上一个$$T^2$$，这是为了让损失函数的两项的梯度大致在一个数量级上(参考公式 (9))，实验表明这将大大改善新模型的表现(考虑到加入了更多的监督信号)。</p>
<h2 id="与直接优化logits差异相比"><a href="#与直接优化logits差异相比" class="headerlink" title="与直接优化logits差异相比"></a>与直接优化logits差异相比</h2><p>由公式(2)(3)，对于交叉熵损失来说，其对于新模型的某个logit $$z_i$$ 的梯度是</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_i%7D&=%5Cfrac%7B1%7D%7BT%7D(q_i-p_i)%5Ctag%7B4%7D%5C%5C+&=%5Cfrac%7B1%7D%7BT%7D%5Cleft(%5Cfrac%7B%5Cexp(z_i/T)%7D%7B%5Csum_j%5Cexp(z_j/T)%7D-+%5Cfrac%7B%5Cexp(v_i/T)%7D%7B%5Csum_j%5Cexp(v_j/T)%7D%5Cright)%5Ctag%7B5%7D+%5Cend%7Balign%7D" alt="[公式]"></p>
<p>由于$$ e^x-1$$ 与 $$x$$ 是<strong>等价无穷小(</strong>$$x \to 0$$ 时**)**，易知，当$$T$$充分大时，有</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_i%7D&%5Capprox%5Cfrac%7B1%7D%7BT%7D%5Cleft(%5Cfrac%7B1+z_i/T%7D%7B%5Csum_j(1+z_j/T)%7D-%5Cfrac%7B1+v_i/T%7D%7B%5Csum_j(1+v_j/T)%7D%5Cright)%5Ctag%7B6%7D%5C%5C+&=%5Cfrac%7B1%7D%7BT%7D%5Cleft(%5Cfrac%7B1+z_i/T%7D%7BN+%5Csum_jz_j/T%7D-%5Cfrac%7B1+v_i/T%7D%7BN+%5Csum_jv_j/T%7D%5Cright)%5Ctag%7B7%7D%5C%5C+%5Cend%7Balign%7D" alt="[公式]"></p>
<p>假设所有logits对每个样本都是零均值化的，即 <img src="https://www.zhihu.com/equation?tex=%5Csum_jz_j=%5Csum_jv_j=0" alt="[公式]"> ，则有</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_i%7D&%5Capprox%5Cfrac%7B1%7D%7BT%7D%5Cleft(%5Cfrac%7B1+z_i/T%7D%7BN%7D-%5Cfrac%7B1+v_i/T%7D%7BN%7D%5Cright)%5Ctag%7B8%7D%5C%5C+&=%5Cfrac%7B1%7D%7BNT%5E2%7D%5Cleft(z_i-v_i%5Cright)%5Ctag%7B9%7D+%5Cend%7Balign%7D" alt="[公式]"></p>
<p>所以，如果：1. $$T$$非常大，2. logits对所有样本都是零均值化的，则知识蒸馏和最小化logits的平方差(公式 (1) )是等价的(因为梯度大致是同一个形式)。实验表明，温度 $$T$$ 不能取太大，而应该使用某个适中的值，这表明忽略极负的logits对新模型的表现很有帮助(较低的温度产生的分布比较「硬」，倾向于忽略logits中极小的负值)。</p>
<h2 id="实验与结论"><a href="#实验与结论" class="headerlink" title="实验与结论"></a>实验与结论</h2><p>Hinton等人做了三组实验，其中两组都验证了知识蒸馏方法的有效性。在MNIST数据集上的实验表明，即便有部分类别的样本缺失，新模型也可以表现得很不错，只需要修改相应的偏置项，就可以与原模型表现相当。在语音任务的实验也表明，蒸馏得到的模型比从头训练的模型捕捉了更多数据集中的有效信息，表现仅比集成模型低了0.3个百分点。总体来说知识蒸馏是一个简单而有效的模型压缩&#x2F;训练方法。这大体上是因为原模型的softmax提供了比one-hot标签更多的监督信号[3]。</p>
<p>知识蒸馏在后续也有很多延伸工作。在NLP方面比较有名的有Yoon Kim等人的<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1606.07947">Sequence-Level Knowledge Distillation</a> 等。总的来说，对一些比较臃肿、不便部署的模型，可以将其「知识」转移到小的模型上。比如，在机器翻译中，一般的模型需要有较大的容量(capacity)才可能获得较好的结果；现在非常流行的BERT及其变种，规模都非常大；更不用提，一些情形下我们需要将这些本身就很大的深度模型集成为一个ensemble，这时候，可以用知识蒸馏压缩出一个较小的、「便宜」的模型。</p>
<p>另外，在多任务的情境下，使用一般的策略训练一个多任务模型，可能达不到比单任务更好的效果，文献[3]探索了使用知识蒸馏，利用单任务的模型来指导训练多任务模型的方法，很值得参考。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>(4)梯度的推导。</p>
<p>由链式法则，有</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z%7D=%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+z%7D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+q%7D%5Ctag%7B10%7D" alt="[公式]"></p>
<p>注意到 $$p$$ 是原模型产生的softmax输出，与 $$ z$$ 无关。</p>
<p>后一项 $$\partial C / \partial q $$ 比较容易得到，因为 </p>
<p><img src="https://www.zhihu.com/equation?tex=C=%5Csum_%7Bi=1%7D%5E%7Bn%7D-p_i%5Clog+q_i" alt="[公式]"> </p>
<p>所以</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+q_i%7D=-%5Cfrac%7Bp_i%7D%7Bq_i%7D%5Ctag%7B11%7D" alt="[公式]"></p>
<p>则 $$\partial C / \partial q $$是一个 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 维向量</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+q%7D=%5Cleft%5B%5Cbegin%7Bmatrix%7D+-%5Cfrac%7Bp_1%7D%7Bq_1%7D%5C%5C+-%5Cfrac%7Bp_2%7D%7Bq_2%7D%5C%5C+%5Cvdots%5C%5C+-%5Cfrac%7Bp_n%7D%7Bq_n%7D++%5Cend%7Bmatrix%7D%5Cright%5D%5Ctag%7B12%7D" alt="[公式]"></p>
<p>前一项 $$\partial q / \partial z$$是一个 $$n\times n$$ 的方阵，分类讨论可以得到。参考公式 (2)，记</p>
<p> <img src="https://www.zhihu.com/equation?tex=Z=%5Csum_k+%5Cexp(z_k/T)" alt="[公式]"> </p>
<p>由除法的求导法则，输出元素 $$q_i$$ 对输入 $$z_j$$ 的偏导是</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+q_i%7D%7B%5Cpartial+z_j%7D=%5Cfrac%7B1%7D%7BZ%5E2%7D%5Cleft(Z%5Cfrac%7B%5Cpartial+%5Cexp(z_i/T)%7D%7B%5Cpartial+z_j%7D-%5Cexp(z_i/T)%5Cboxed%7B%5Cfrac%7B%5Cpartial+Z%7D%7B%5Cpartial+z_j%7D%7D++%5Cright)%5Ctag%7B13%7D" alt="[公式]"></p>
<p>注意上面右侧加方框部分，可以进一步展开</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+Z%7D%7B%5Cpartial+z_j%7D=%5Cfrac%7B1%7D%7BT%7D%5Cexp(z_j/T)%5Ctag%7B14%7D" alt="[公式]"></p>
<p>这样，代入公式 <img src="https://www.zhihu.com/equation?tex=(13)" alt="[公式]"> ，并且将括号展开，可以得到</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cfrac%7B%5Cpartial+q_i%7D%7B%5Cpartial+z_j%7D&=%5Cfrac%7B1%7D%7BZ%7D%5Cfrac%7B%5Cpartial+%5Cexp(z_i/T)%7D%7B%5Cpartial+z_j%7D-%5Cfrac%7B1%7D%7BTZ%5E2%7D%5Cexp(z_i/T)%5Cexp(z_j/T)%5Ctag%7B15%7D%5C%5C+&=%5Cfrac%7B1%7D%7BZ%7D%5Cfrac%7B%5Cpartial+%5Cexp(z_i/T)%7D%7B%5Cpartial+z_j%7D-%5Cfrac%7B1%7D%7BT%7D%5Cfrac%7B%5Cexp(z_i/T)%7D%7BZ%7D%5Cfrac%7B%5Cexp(z_j/T)%7D%7BZ%7D%5Ctag%7B16%7D%5C%5C+&=%5Cfrac%7B1%7D%7BZ%7D%5Cboxed%7B%5Cfrac%7B%5Cpartial+%5Cexp(z_i/T)%7D%7B%5Cpartial+z_j%7D%7D-%5Cfrac%7B1%7D%7BT%7Dq_iq_j%5Ctag%7B17%7D%5C%5C+%5Cend%7Balign%7D" alt="[公式]"></p>
<p>左侧方框内偏导可以分类讨论得到</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cexp(z_i/T)%7D%7B%5Cpartial+z_j%7D=%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7BT%7D%5Cexp(z_i/T),%5C+&%5Ctext%7Bif+%7Di=j%5C%5C+0,&%5Ctext%7Bif+%7Di%5Cneq+j+%5Cend%7Bcases%7D%5Ctag%7B18%7D" alt="[公式]"></p>
<p>带入式 (17)，得到</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cfrac%7B%5Cpartial+q_i%7D%7B%5Cpartial+z_j%7D+&=%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7BT%7D%5Cleft(%5Cfrac%7B%5Cexp(z_i/T)%7D%7BZ%7D-q_iq_j%5Cright)+,%5C+&%5Ctext%7Bif+%7Di=j%5C%5C+-%5Cfrac%7B1%7D%7BT%7Dq_iq_j,&%5Ctext%7Bif+%7Di%5Cneq+j+%5Cend%7Bcases%7D%5Ctag%7B19%7D%5C%5C+&=%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7BT%7D%5Cleft(q_i-q_iq_j%5Cright)+,%5C+&%5Ctext%7Bif+%7Di=j%5C%5C+-%5Cfrac%7B1%7D%7BT%7Dq_iq_j,&%5Ctext%7Bif+%7Di%5Cneq+j+%5Cend%7Bcases%7D%5Ctag%7B20%7D+%5Cend%7Balign%7D" alt="[公式]"></p>
<p>所以 $$\partial q / \partial z$$形式如下</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+z%7D=%5Cfrac%7B1%7D%7BT%7D%5Cleft%5B+%5Cbegin%7Bmatrix%7D+q_1-q_1%5E2+&+-q_1q_2+&+%5Ccdots+&+-q_1q_n%5C%5C+-q_2q_1+&+q_2-q_2%5E2+&+%5Ccdots+&+-q_2q_n%5C%5C+%5Cvdots+&+%5Cvdots+&+%5Cddots+&%5Cvdots%5C%5C+-q_nq_1+&+-q_nq_2+&+%5Ccdots+&+q_n-q_n%5E2+%5Cend%7Bmatrix%7D+%5Cright%5D%5Ctag%7B21%7D" alt="[公式]"></p>
<p>代入式(10)，可得</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z%7D&=%5Cfrac%7B1%7D%7BT%7D%5Cleft%5B+%5Cbegin%7Bmatrix%7D+q_1-q_1%5E2+&+-q_1q_2+&+%5Ccdots+&+-q_1q_n%5C%5C+-q_2q_1+&+q_2-q_2%5E2+&+%5Ccdots+&+-q_2q_n%5C%5C+%5Cvdots+&+%5Cvdots+&+%5Cddots+&%5Cvdots%5C%5C+-q_nq_1+&+-q_nq_2+&+%5Ccdots+&+q_n-q_n%5E2+%5Cend%7Bmatrix%7D+%5Cright%5D+%5Cleft%5B%5Cbegin%7Bmatrix%7D+-%5Cfrac%7Bp_1%7D%7Bq_1%7D%5C%5C+-%5Cfrac%7Bp_2%7D%7Bq_2%7D%5C%5C+%5Cvdots%5C%5C+-%5Cfrac%7Bp_n%7D%7Bq_n%7D++%5Cend%7Bmatrix%7D%5Cright%5D%5Ctag%7B22%7D%5C%5C++&=%5Cfrac%7B1%7D%7BT%7D%5Cleft%5B%5Cbegin%7Bmatrix%7D+-p_1+%5Csum_kp_kq_1%5C%5C+-p_2+%5Csum_kp_kq_2%5C%5C+%5Cvdots%5C%5C+-p_n+%5Csum_kp_kq_n++%5Cend%7Bmatrix%7D%5Cright%5D%5Ctag%7B23%7D%5C%5C++&=%5Cfrac%7B1%7D%7BT%7D%5Cleft%5B%5Cbegin%7Bmatrix%7D+-p_1+q_1%5C%5C+-p_2+q_2%5C%5C+%5Cvdots%5C%5C+-p_n+q_n+%5Cend%7Bmatrix%7D%5Cright%5D%5Ctag%7B24%7D%5C%5C+&=%5Cfrac%7B1%7D%7BT%7D(q-p)%5Ctag%7B25%7D+%5Cend%7Balign%7D" alt="[公式]"></p>
<p>所以有公式(4) ， <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z_i%7D=%5Cfrac%7B1%7D%7BT%7D(q_i-p_i)" alt="[公式]"> 。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">re-burn</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://re-burn.github.io/2022/04/12/知识蒸馏/">https://re-burn.github.io/2022/04/12/知识蒸馏/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://re-burn.github.io">re-burn 笔记堆</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><a class="post-meta__tags" href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">知识蒸馏</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2022/04/12/ViT/"><span>Vision Transformer</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2022 By re-burn</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>