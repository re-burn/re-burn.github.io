<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Two-Stream Inflated 3D ConvNets"><meta name="keywords" content="计算机视觉,I3D,行为识别"><meta name="author" content="re-burn"><meta name="copyright" content="re-burn"><title>Two-Stream Inflated 3D ConvNets | re-burn 笔记堆</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Two-Stream-Inflated-3D-ConvNets"><span class="toc-number">1.</span> <span class="toc-text">Two-Stream Inflated 3D ConvNets</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE"><span class="toc-number">1.1.</span> <span class="toc-text">1 主要贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">2 主要方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%9C%AC%E6%96%87%E5%AE%9E%E9%AA%8C%E7%9A%84%E5%85%B6%E4%BB%96%E8%A7%86%E9%A2%91%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 本文实验的其他视频行为识别模型设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Two-Stream-Inflated-3D-ConvNets-%EF%BC%88I3D%EF%BC%89-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 Two-Stream Inflated 3D ConvNets （I3D） 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E8%AE%AD%E7%BB%83"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BB%86%E8%8A%82%E9%97%AE%E9%A2%98"><span class="toc-number">1.3.</span> <span class="toc-text">3 细节问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2D%E5%8D%B7%E7%A7%AF%E6%A0%B8%EF%BC%88%E5%92%8C%E6%9D%83%E9%87%8D%EF%BC%89%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E5%A4%8D%E5%88%B6N%E6%AC%A1%E5%BE%97%E5%88%B03D%E5%8D%B7%E7%A7%AF%E6%A0%B8%EF%BC%9F3D%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%9D%83%E9%87%8D%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%99%A4%E4%BB%A5N%EF%BC%9F"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 2D卷积核（和权重）为什么可以直接复制N次得到3D卷积核？3D卷积的权重为什么要除以N？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%BD%9C%E8%80%85%E4%BD%BF%E7%94%A8%E5%9F%BA%E4%BA%8E3D%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A83D%E7%BD%91%E7%BB%9C%EF%BC%8C%E8%80%8C%E6%98%AF%E5%BC%95%E5%85%A5%E9%A2%9D%E5%A4%96%E7%9A%84%E5%85%89%E6%B5%81%E8%BE%93%E5%85%A5%EF%BC%9F"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 作者使用基于3D卷积的双流网络，为什么不直接使用3D网络，而是引入额外的光流输入？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.4.</span> <span class="toc-text">4 实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Imagenet%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 Imagenet预训练</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">re-burn</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">8</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">9</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">4</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">re-burn 笔记堆</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">Two-Stream Inflated 3D ConvNets</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-04-12</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">日常学习笔记</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Two-Stream-Inflated-3D-ConvNets"><a href="#Two-Stream-Inflated-3D-ConvNets" class="headerlink" title="Two-Stream Inflated 3D ConvNets"></a>Two-Stream Inflated 3D ConvNets</h1><blockquote>
<p>Paper：<a href="https://link.zhihu.com/?target=https://openaccess.thecvf.com/content_cvpr_2017/papers/Carreira_Quo_Vadis_Action_CVPR_2017_paper.pdf">Quo Vadis, action recognition? A new model and the kinetics dataset</a></p>
</blockquote>
<h2 id="1-主要贡献"><a href="#1-主要贡献" class="headerlink" title="1 主要贡献"></a>1 主要贡献</h2><p><strong>作者的Motivation主要是为了解决两个问题：</strong></p>
<p>（1）现有的数据集，如UCF-101和HMDB-51的视频数量都比较少，很多模型因此都获得了比较接近的效果，没法有效的对模型性能进行评价（如，我们在mnist数据集上，可能自己随便搭个三五层网络，和efficinet-b7的准确率可能都是0.99+，没有办法有效的评价不同模型的好坏）。</p>
<p>为什么作者想要对之前的模型进行比较？对于视频行为识别来说，不同于图像任务，各种方法层出不穷，如<strong>Fig. 2</strong> 所示。输入数据可以有RGB视频或者额外的光流，网络结构有3D卷积、双流、2D卷积+LSTM等等。谁也不好说哪种方法更好。</p>
<p><img src="https://pic4.zhimg.com/80/v2-fef44f03cb1ac9a5910d8f5ba575c3a7_720w.jpg" alt="img">Fig. 2 多种视频行为识别方案</p>
<p>（2）Imagenet图像分类中出现了很多优秀的网络结构，并且这些模型和他们在Imagenet上的预训练权重能够很好的扩展应用到其他领域，如目标检测，分割等等。但是这些任务能够通过预训练+微调的方式获得一定的提升，是不是因为他们都是在Spatial领域呢？对于视频行为识别来说，多了一个Temporal领域，预训练模型能不能仍然work呢？</p>
<p><strong>针对1.1中的两个问题，作者的解决方案包括：</strong></p>
<p>（1） 针对问题（1），作者提出了一个新的大型视频行为识别数据集 <strong>Kinetics</strong> Human Action Video dataset。这个数据集比UCF-101和HMDB-51大了两个数量级。包括400个人类动作的类别，每个类别都有超过400个clips。数据集是从Youtube中搜集的真实场景下的视频。</p>
<p>（2）对于视频行为识别任务来说，如 <strong>Fig. 3</strong> 所示，相较于图像分类等任务，多了一个时间维度。而Imagenet挑战赛中的各种2D卷积网络没有办法同时提取时空信息，怎么把2D卷积的权重，扩展到3D卷积上呢？作者提出了一种方法（很简单，后面具体介绍），并且基于这种方法，提出了<strong>Two-Stream Inflated 3D ConvNets (I3D)</strong> 模型。</p>
<p><img src="https://pic3.zhimg.com/80/v2-ae9afcc13e21f9c7df5ed852df3bd3c2_720w.jpg" alt="img">Fig. 3 2D卷积核的预训练权重怎么应用到3D卷积核上？</p>
<p>（3）既然可以尝试把Imagenet中的预训练权重扩展到视频行为识别中来，那么在视频行为识别中，作者也实验了在大型的数据集（作者提出的Kinetics数据集）上预训练，然后在UCF-101和HMDB-51上微调的方案。</p>
<h2 id="2-主要方法"><a href="#2-主要方法" class="headerlink" title="2 主要方法"></a>2 主要方法</h2><h3 id="2-1-本文实验的其他视频行为识别模型设置"><a href="#2-1-本文实验的其他视频行为识别模型设置" class="headerlink" title="2.1 本文实验的其他视频行为识别模型设置"></a>2.1 本文实验的其他视频行为识别模型设置</h3><p><strong>2.1.1 2D CNN + LSTM</strong></p>
<p>由于2D分类网络的效果好，想到把其用在video上。使用2D CNN的好处是可以直接从Imagenet的预训练权重迁移过来，但是只用那些2D卷积网络不能捕捉时空方面的信息，因此使用LSTM提取时序特征。流程是，首先使用2D网络分别提取每一帧图像的特征，然后对于所有特征使用LSTM来获取帧之间的时序信息。</p>
<p>作者使用Inception-V1作为2D CNN骨干网络，把最后一个平均池化输出的512维特征向量输入作为LSTM一个step的输入，再使用一个全连接层作为分类器。</p>
<p>使用交叉熵损失，训练时对于每一个step的输出都计算Loss。测试时使用最后一个step的输出类别作为结果。</p>
<p>这种方法的<strong>优点</strong>是可以直接使用2D网络的预训练权重；<strong>缺点</strong>是只有在LSTM的部分才能够基于高维抽象的特征进行运动信息的提取，损失了很多低等级的运动信息，此外LSTM训练时需要在每个step都进行反向传播。</p>
<p><strong>2.1.2 3D CNN</strong></p>
<p>如果说2D卷积是解决各种图像任务的天选之子，那么3D卷积就是提取时空特征时最自然的解决方案，如C3D，可以参考文章：</p>
<p>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/443813567">论文笔记] C3D - 知乎 (zhihu.com)</a></p>
<p>其<strong>优点</strong>就是可以同时提取时空信息；<strong>缺点</strong>也显而易见，即具有更庞大的参数数量从而更难训练，因此一般3D网络的深度都较浅，单这样又影响了模型的表达能力，此外，没有办法能够有效的把2D网络的预训练权重迁移到3D网络。</p>
<p>本文作者在C3D的基础上进行了改进：（1）在所有卷积和全连接层之后都假如了BN，（2）原始C3D的第一个池化步长是 $$ 1 \times 2 \times 2 $$ ，本文改为了  $$ 2 \times 2 \times 2 $$ ，因此能够更加节省显存占用（可以使用更大的Batch，对于BN来说这也是非常重要的)。</p>
<p><strong>2.1.3 Two-Stream</strong></p>
<p>双流网络中一个分支作为spatial flow，输入RGB图像来提取物体和场景外观特征；另一个分支作为temporal flow，输入光流来提取运动特征。通常两个网络分开训练，只有在测试时才会平均两个网络的预测（测试的时候是输入很多个单张图，最后结果取平均）。因此，也产生了时空信息在浅层网络中无法有效融合的问题。</p>
<p><strong>2.1.4 3D-fused Two-stream</strong></p>
<p>双流的一个改进：在最后一层卷积层之后，使用3D ConvNet把空间流和时间流<strong>融合</strong>（相比于传统双流是在softmax后才做fusion，把softmax输出的score进行平均）。输入网络的是相隔10帧采样的5个连续RGB帧。</p>
<h3 id="2-2-Two-Stream-Inflated-3D-ConvNets-（I3D）-网络结构"><a href="#2-2-Two-Stream-Inflated-3D-ConvNets-（I3D）-网络结构" class="headerlink" title="2.2 Two-Stream Inflated 3D ConvNets （I3D） 网络结构"></a><strong>2.2 Two-Stream Inflated 3D ConvNets （I3D） 网络结构</strong></h3><p>作者使用Inception-V1[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/445736110#ref_1">1]</a>作为骨干网络，其网络结构如图所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-1d6b29eff4be9ca7c1349ffe440d1b7d_720w.jpg" alt="img">Fig. 4 Inception-V1网络结构</p>
<p><strong>2.2.1 2D网络到3D网络</strong></p>
<p>Imagenet挑战赛中有很多优秀的模型，想使用他们的3D版本包括两个步骤：（1）模型从2D变成3D，（2）权重从2D变成3D。</p>
<p>对于模型，本文直接把2D卷积或2D池化的 $$ N \times N $$ 的kernel变成  $$ N \times N \times N $$ 的kernel。</p>
<p>对于预训练权重，首先把2D卷积核在时间维度上复制N份，然后除以时间维度的维度N，这样做是为了扩展到3D卷积之后，每一层都仍然获取到类似大小的输出相应。如果不除以N，就会出现输出相应会增大N倍，改变了预训练时网络所学习到的数据分布。</p>
<p><strong>2.2.2 池化窗口、步长和卷积步长应该怎么设置？</strong></p>
<p>我们已经把2D卷积核扩展成为了3D卷积核。但是2D池化窗口 $$k \times k$$ ，2D池化步长 $$ s_p \times s_p $$，2D卷积核的步长 $$s_c \times s_c$$ 应该怎么扩展到3D？</p>
<p>一个自然的想法是向卷积核一样，把2D  $$k \times k$$  的池化窗口直接变成  $$k \times k \times k$$  ，同理步长也都直接对称的扩展。但是考虑输入图像的高宽，以及时间维度的尺度差异，如果简单的在时间维度也使用相同的池化窗口，则会导致时间维度过快的变化，可能在浅层网络中把不同时间中的边缘特征进行过早的混合，影响空间特征的提取；而如果时间维度的窗口或步长太小，则可能导致不能有效的提取空间动态信息。</p>
<p>至于怎么设置池化窗口、步长，还有卷积的步长，主要依赖于视频帧率和输入分辨率。本文作者在Inception-V1中的设置为（如<strong>Fig. 1</strong>所示）：</p>
<ul>
<li>前两个池化窗口为 $$3 \times 3$$ ，步长 $$2 \times 2$$ 的max pool扩展为池化窗口为  ，$$1 \times 3 \times 3$$步长为  。即$$1 \times 2 \times 2$$不过早的增大时间维度感受野，浅层网络中更加倾向于提取空间特征。</li>
<li>中间两个池化窗口为$$3 \times 3$$，步长 $$2 \times 2$$的max pool扩展为池化窗口为  $$3 \times 3 \times 3$$，步长为 $$2 \times 2 \times 2 $$。</li>
<li>最后一个avg pool的池化窗口扩展为 $$2 \times 7 \times 7$$ 。</li>
<li>Inception Module中的池化都扩展为和高、宽维度相同的窗口大小、步长。</li>
</ul>
<h3 id="2-3-训练"><a href="#2-3-训练" class="headerlink" title="2.3 训练"></a>2.3 训练</h3><ul>
<li>双流的两个分支在训练时分别训练，在测试时取平均。</li>
<li>对于所有的卷积层，都由一个BN和ReLU。</li>
<li>SGD + momentum&#x3D;0.9</li>
<li>把视频最短边resize到256，然后random crop $$224 \times 224$$ ，随机水平翻转。</li>
<li>如果视频较短，循环视频知道满足输入。</li>
<li>使用TV-L1算法计算光流。</li>
</ul>
<h2 id="3-细节问题"><a href="#3-细节问题" class="headerlink" title="3 细节问题"></a>3 细节问题</h2><h3 id="3-1-2D卷积核（和权重）为什么可以直接复制N次得到3D卷积核？3D卷积的权重为什么要除以N？"><a href="#3-1-2D卷积核（和权重）为什么可以直接复制N次得到3D卷积核？3D卷积的权重为什么要除以N？" class="headerlink" title="3.1 2D卷积核（和权重）为什么可以直接复制N次得到3D卷积核？3D卷积的权重为什么要除以N？"></a>3.1 2D卷积核（和权重）为什么可以直接复制N次得到3D卷积核？3D卷积的权重为什么要除以N？</h3><p>我们首先不考虑使用在Imagenet上的预训练权重来把2D卷积扩展称为3D卷积。那么能直接使用3D卷积在Imagenet上预训练吗？</p>
<p>假设我们有一个视频片段，其中的物体移动非常慢，几乎没有运动（比如摄像头拍向一块空地）。我们在这段视频中取一些帧出来，比如取16帧，拼接起来作为3D卷积网络的输入。那么我们可以认为取出来的16帧等价于1帧图像复制16份（视频中几乎没有运动）。</p>
<p>我们按照这个思路来考虑使用Imagenet来直接预训练3D卷积网络，即在Imagenet中取出1张图像，复制16份并拼接作为输入视频（boring video）。然后使用3D卷积来提取特征就可以了。</p>
<p>我们来拆分一下3D卷积核 $$h \times w \times t$$ ，分解成$$t$$个$$h \times w$$的2D卷积核。这里的$$h,w$$分别是卷积核的高宽，  是时间$$t$$维度的深度，也就是我们把imagenet中的1张图像叠加16份的”16”。 $$h \times w$$ 作用在$$t=16$$上的任意帧时，所提取的空间特征都应该是完全相同的，因为叠加的每一帧都是1张图像复制来的。</p>
<p>所以把2D卷积核复制N次得到3D卷积核，等价于把1张静态图像复制拼接为boring video，然后使用3D卷积核去预训练是等价的。</p>
<p>但是，由2D卷积核预训练权重复制得到的3D卷积核，为什么权重还要除以N？</p>
<p>还是考虑把imagenet中的1帧图像复制16份拼接成一个boring video。使用2D卷积时，其权重假设分别为:</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin+%7Barray%7D%7B1%7D+0.1+&+0.2+&+0.3+%5C%5C+0.4+&+0.5+&+0.6+%5C%5C+0.7+&+0.8+&+0.9+%5C%5C+%5Cend+%7Barray%7D" alt="[公式]"> </p>
<p>提取特征图上的一块 $$3 \times 3$$ 区域的像素</p>
<p> <img src="https://www.zhihu.com/equation?tex=%5Cbegin+%7Barray%7D%7B1%7D+x_1+&+x_2+&+x_3+%5C%5C+x_4+&+x_5+&+x_6+%5C%5C+x_7+&+x_8+&+x_9+%5C%5C+%5Cend+%7Barray%7D" alt="[公式]"> </p>
<p>得到1个输出值</p>
<p> <img src="https://www.zhihu.com/equation?tex=x_%7Bout%7D+=+0.1x_1+++0.2x_2+++0.3x_3+++0.4x_4+++...+++0.9x_9" alt="[公式]"></p>
<p>如果使用通过2D卷积复制来的3D卷积时，得到的1个输出值为： <img src="https://www.zhihu.com/equation?tex=x_%7Bout%7D+=+3+%5Ctimes+(0.1x_1+++0.2x_2+++0.3x_3+++0.4x_4+++...+++0.9x_9)" alt="[公式]"> </p>
<p>则改变了下一层卷积的输入特征响应激活值了，所以要对2D卷积核的权重进行归一化， 即除以N。</p>
<h3 id="3-2-作者使用基于3D卷积的双流网络，为什么不直接使用3D网络，而是引入额外的光流输入？"><a href="#3-2-作者使用基于3D卷积的双流网络，为什么不直接使用3D网络，而是引入额外的光流输入？" class="headerlink" title="3.2 作者使用基于3D卷积的双流网络，为什么不直接使用3D网络，而是引入额外的光流输入？"></a>3.2 作者使用基于3D卷积的双流网络，为什么不直接使用3D网络，而是引入额外的光流输入？</h3><p><img src="https://pic2.zhimg.com/80/v2-4e82002d38eed0231c0e382abf0a5201_720w.jpg" alt="img"></p>
<h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4 实验结果"></a>4 实验结果</h2><h3 id="4-1-Imagenet预训练"><a href="#4-1-Imagenet预训练" class="headerlink" title="4.1 Imagenet预训练"></a>4.1 Imagenet预训练</h3><p><img src="https://pic1.zhimg.com/80/v2-99f39d00e26bd8af45b85aa85009f60c_720w.jpg" alt="img"></p>
<p>可以发现，ImageNet then Kinetics中，准确率都有提高。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">re-burn</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://re-burn.github.io/2022/04/12/I3D/">https://re-burn.github.io/2022/04/12/I3D/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://re-burn.github.io">re-burn 笔记堆</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><a class="post-meta__tags" href="/tags/I3D/">I3D</a><a class="post-meta__tags" href="/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/">行为识别</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/04/12/transformer/"><i class="fa fa-chevron-left">  </i><span>transformer</span></a></div><div class="next-post pull-right"><a href="/2022/04/12/attention/"><span>Attention</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2022 By re-burn</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>