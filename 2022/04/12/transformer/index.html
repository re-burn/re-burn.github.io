<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="transformer"><meta name="keywords" content="transformer,深度学习"><meta name="author" content="re-burn"><meta name="copyright" content="re-burn"><title>transformer | re-burn 笔记堆</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer"><span class="toc-number">1.</span> <span class="toc-text">transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="toc-number">1.0.1.</span> <span class="toc-text">代码：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8F%E8%A7%82%E7%BB%93%E6%9E%84%EF%BC%9A"><span class="toc-number">1.0.2.</span> <span class="toc-text">宏观结构：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E5%BC%A0%E9%87%8F%E5%BC%95%E5%85%A5%E5%9B%BE%E6%99%AF"><span class="toc-number">1.0.3.</span> <span class="toc-text">将张量引入图景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.0.4.</span> <span class="toc-text">自注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E5%AE%9E%E7%8E%B0%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.0.5.</span> <span class="toc-text">通过矩阵运算实现自注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.0.6.</span> <span class="toc-text">多头注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E8%A1%A8%E7%A4%BA%E5%BA%8F%E5%88%97%E7%9A%84%E9%A1%BA%E5%BA%8F"><span class="toc-number">1.0.7.</span> <span class="toc-text">使用位置编码表示序列的顺序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E6%A8%A1%E5%9D%97"><span class="toc-number">1.1.</span> <span class="toc-text">残差模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E7%BB%84%E4%BB%B6"><span class="toc-number">1.1.1.</span> <span class="toc-text">解码组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E7%9A%84%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E5%92%8CSoftmax%E5%B1%82"><span class="toc-number">1.1.2.</span> <span class="toc-text">最终的线性变换和Softmax层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%83%A8%E5%88%86%E6%80%BB%E7%BB%93"><span class="toc-number">1.1.3.</span> <span class="toc-text">训练部分总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.4.</span> <span class="toc-text">损失函数</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">re-burn</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">8</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">9</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">4</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">re-burn 笔记堆</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">transformer</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-04-12</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">日常学习笔记</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1><p>谷歌的Transformer模型最早是用于机器翻译任务，当时达到了SOTA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。</p>
<h3 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h3><p>Attention is All You Need：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<h3 id="宏观结构："><a href="#宏观结构：" class="headerlink" title="宏观结构："></a>宏观结构：</h3><p>transformer由编码组件、解码组件和它们之间的连接组成。</p>
<p>编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。</p>
<p><img src="https://n.sinaimg.cn/front/9/w946h663/20190108/P-K9-hrkkwef7008787.jpg" alt="img"></p>
<p>所有的编码器在结构上都是相同的，但它们没有共享参数。每个编码器都可以分解成两个子层：</p>
<p><img src="https://n.sinaimg.cn/front/719/w1051h468/20190108/X7dZ-hrkkwef7008863.jpg" alt="img"></p>
<p>从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。我们将在稍后的文章中更深入地研究自注意力。</p>
<p>自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。</p>
<p>解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。</p>
<p><img src="https://n.sinaimg.cn/front/610/w1080h330/20190108/YmrV-hrkkwef7008971.jpg" alt="img"></p>
<h3 id="将张量引入图景"><a href="#将张量引入图景" class="headerlink" title="将张量引入图景"></a>将张量引入图景</h3><p>我们已经了解了模型的主要部分，接下来我们看一下各种向量或张量（译注：张量概念是矢量概念的推广，可以简单理解矢量是一阶张量、矩阵是二阶张量。）是怎样在模型的不同部分中，将输入转化为输出的。</p>
<p>像大部分NLP应用一样，我们首先将每个输入单词通过词嵌入算法转换为词向量。</p>
<p><img src="https://n.sinaimg.cn/sinacn20116/123/w824h99/20190108/f85a-hrkkwef7014602.png" alt="img"></p>
<p>每个单词都被嵌入为512维的向量，我们用这些简单的方框来表示这些向量。</p>
<p>词嵌入过程只发生在最底层的编码器中。所有的编码器都有一个相同的特点，即它们接收一个向量列表，列表中的每个向量大小为512维。在底层（最开始）编码器中它就是词向量，但是在其他编码器中，它就是上一层编码器的输出（也是一个向量列表）。向量列表大小是我们可以设置的超参数——一般是我们训练集中最长句子的长度。</p>
<p>将输入序列进行词嵌入之后，每个单词都会流经编码器中的两个子层。<br><img src="https://n.sinaimg.cn/sinacn20116/71/w1016h655/20190108/a2c2-hrkkwef7014781.jpg" alt="img"></p>
<p>接下来我们看看Transformer的一个核心特性，在这里输入序列中每个位置的单词都有自己独特的路径流入编码器。在自注意力层中，这些路径之间存在依赖关系。而前馈（feed-forward）层没有这些依赖关系。因此在前馈（feed-forward）层时可以并行执行各种路径。</p>
<h3 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h3><p>例如，下列句子是我们想要翻译的输入句子：</p>
<p>The animal didn’t cross the street because it was too tired</p>
<p>这个“it”在这个句子是指什么呢？它指的是street还是这个animal呢？这对于人类来说是一个简单的问题，但是对于算法则不是。</p>
<p>当模型处理这个单词“it”的时候，自注意力机制会允许“it”与“animal”建立联系。</p>
<p>随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。</p>
<p>首先我们了解一下如何使用向量来计算自注意力，然后来看它实怎样用矩阵来实现。</p>
<p>计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。</p>
<p>可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入&#x2F;输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。</p>
<p><img src="https://n.sinaimg.cn/sinacn20116/96/w1080h616/20190108/5bcc-hrkkwef7014930.jpg" alt="img"></p>
<p>X<del>1</del>与W^Q^权重矩阵相乘得到q1, 就是与这个单词相关的查询向量。以此类推，最终使得输入序列的每个单词的创建一个查询向量、一个键向量和一个值向量。</p>
<p>计算自注意力的第二步是计算得分。假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其它部分。</p>
<p>这些分数是通过打分所有单词的键向量与“Thinking”的查询向量相点积来计算的。所以如果我们是处理位置最靠前的词的自注意力的话，第一个分数是q1和k1的点积，第二个分数是q1和k2的点积。<br><img src="https://n.sinaimg.cn/sinacn20116/697/w970h527/20190108/920b-hrkkwef7015202.jpg" alt="img"></p>
<p>第三步和第四步是将分数除以8(8是论文中使用的键向量的维数64的平方根，这会让梯度更稳定。这里也可以使用其它值，8只是默认值)，然后通过softmax传递结果。softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1。</p>
<p><img src="https://n.sinaimg.cn/sinacn20116/22/w943h679/20190108/602d-hrkkwef7015373.jpg" alt="img"></p>
<p>这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。显然，已经在这个位置上的单词将获得最高的softmax分数，但有时关注另一个与当前单词相关的单词也会有帮助。</p>
<p>第五步是将每个值向量乘以对应key的softmax分数。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。</p>
<p>第六步是对加权值向量求和（译注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。），然后即得到自注意力层在该位置的输出(在我们的例子中是对于第一个单词)。<br><img src="https://n.sinaimg.cn/sinacn20116/669/w746h723/20190108/ad95-hrkkwef7015564.jpg" alt="img"></p>
<p>这样自注意力的计算就完成了。得到的向量就可以传给前馈神经网络。然而实际中，这些计算是以矩阵形式完成的，以便算得更快。那我们接下来就看看如何用矩阵实现的。</p>
<h3 id="通过矩阵运算实现自注意力机制"><a href="#通过矩阵运算实现自注意力机制" class="headerlink" title="通过矩阵运算实现自注意力机制"></a>通过矩阵运算实现自注意力机制</h3><p>第一步是计算查询矩阵、键矩阵和值矩阵。为此，我们将将输入句子的词嵌入装进矩阵X中，将其乘以我们训练的权重矩阵(W^Q^，W^K^，W^V^)。</p>
<p>x矩阵中的每一行对应于输入句子中的一个单词。我们再次看到词嵌入向量 (512，或图中的4个格子)和q&#x2F;k&#x2F;v向量(64，或图中的3个格子)的大小差异。</p>
<p>最后，由于我们处理的是矩阵，我们可以将步骤2到步骤6合并为一个公式来计算自注意力层的输出。<br><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" alt="img"></p>
<h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><p>通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：</p>
<p>1.它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。</p>
<p>2.它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询&#x2F;键&#x2F;值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器&#x2F;解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器&#x2F;解码器的向量)投影到不同的表示子空间中。</p>
<p>在“多头”注意机制下，我们为每个头保持独立的查询&#x2F;键&#x2F;值权重矩阵，从而产生不同的查询&#x2F;键&#x2F;值矩阵。和之前一样，我们拿X乘以W^Q^&#x2F;W^K^&#x2F;W^V^矩阵来产生查询&#x2F;键&#x2F;值矩阵。</p>
<p>如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵。<br><img src="https://n.sinaimg.cn/sinacn20116/639/w951h488/20190108/8b6b-hrkkwef7015744.png" alt="img"></p>
<p>这给我们带来了一点挑战。前馈层不需要8个矩阵，它只需要一个矩阵(由每一个单词的表示向量组成)。所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵W^O^与它们相乘。</p>
<p><img src="https://n.sinaimg.cn/front/53/w1080h573/20190108/MGNp-hrkkwef7009086.jpg" alt="img"></p>
<p>这几乎就是多头自注意力的全部。这确实有好多矩阵，我们试着把它们集中在一个图片中，这样可以一眼看清。</p>
<p><img src="https://n.sinaimg.cn/front/79/w1080h599/20190108/aEDY-hrkkwef7009206.jpg" alt="img"></p>
<h3 id="使用位置编码表示序列的顺序"><a href="#使用位置编码表示序列的顺序" class="headerlink" title="使用位置编码表示序列的顺序"></a>使用位置编码表示序列的顺序</h3><p>到目前为止，我们对模型的描述缺少了一种理解输入单词顺序的方法。</p>
<p>为了解决这个问题，Transformer为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将位置向量添加到词嵌入中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。</p>
<p><img src="https://n.sinaimg.cn/front/92/w1080h612/20190108/Mjnu-hrkkwef7009436.jpg" alt="img"></p>
<p>为了让模型理解单词的顺序，我们添加了位置编码向量，这些向量的值遵循特定的模式。</p>
<p>如果我们假设词嵌入的维数为4，则实际的位置编码如下：</p>
<h2 id="残差模块"><a href="#残差模块" class="headerlink" title="残差模块"></a>残差模块</h2><p>在继续进行下去之前，我们需要提到一个编码器架构中的细节：在每个编码器中的每个子层（自注意力、前馈网络）的周围都有一个残差连接，并且都跟随着一个“层-归一化”步骤。</p>
<p>层-归一化步骤：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a></p>
<p><img src="https://n.sinaimg.cn/front/8/w960h648/20190108/9QcV-hrkkwef7009734.jpg" alt="img"></p>
<p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p>
<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" alt="img"></p>
<p>解码器的子层也是这样的。如果我们想象一个2层编码-解码结构的transformer，它看起来会像下面这张图一样：</p>
<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" alt="img"></p>
<h3 id="解码组件"><a href="#解码组件" class="headerlink" title="解码组件"></a>解码组件</h3><p>既然我们已经谈到了大部分编码器的概念，那么我们基本上也就知道解码器是如何工作的了。但最好还是看看解码器的细节。</p>
<p>首先，编码器处理输入序列，顶端编码器的输出之后会转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器重点关注输入序列中的合适位置：</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" alt="img"></p>
<p>在完成编码阶段后，即开始解码阶段。解码阶段的每个步骤都会输出一个输出序列中的元素（在这个例子里，输出序列是翻译成英语的句子）</p>
<p>接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示transformer的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像我们对编码器的输入所做的那样，我们会嵌入并添加位置编码给那些解码器，来表示每个单词的位置。</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"><br>而那些解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中前面的那些位置。在softmax步骤前，它会把后面的位置给隐去（把它们设为-inf)。</p>
<p>这个“编码-解码注意力层”工作方式基本就像多头自注意力层一样，只不过它是通过它下方的层来创造查询矩阵，并且从编码器的输出中取得键&#x2F;值矩阵。</p>
<h3 id="最终的线性变换和Softmax层"><a href="#最终的线性变换和Softmax层" class="headerlink" title="最终的线性变换和Softmax层"></a>最终的线性变换和Softmax层</h3><p>解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。</p>
<p>线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。</p>
<p>不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。</p>
<p>接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。</p>
<p><img src="https://n.sinaimg.cn/front/681/w907h574/20190108/ru1--hrkkwef7009838.jpg" alt="img"></p>
<p>这张图片从底部以解码器组件产生的输出向量开始。之后它会转化出一个输出单词。</p>
<h3 id="训练部分总结"><a href="#训练部分总结" class="headerlink" title="训练部分总结"></a>训练部分总结</h3><p>既然我们已经过了一遍完整的transformer的前向传播过程，那我们就可以直观感受一下它的训练过程。</p>
<p>在训练过程中，一个未经训练的模型会通过一个完全一样的前向传播。但因为我们用有标记的训练集来训练它，所以我们可以用它的输出去与真实的输出做比较。</p>
<p>为了把这个流程可视化，不妨假设我们的输出词汇仅仅包含六个单词：“a”, “am”, “i”, “thanks”, “student”以及 “<eos>”（end of sentence的缩写形式）。</p>
<p><img src="https://n.sinaimg.cn/front/592/w1080h312/20190108/-K91-hrkkwef7009953.png" alt="img"></p>
<p>我们模型的输出词表在我们训练之前的预处理流程中就被设定好。</p>
<p>一旦我们定义了我们的输出词表，我们可以使用一个相同宽度的向量来表示我们词汇表中的每一个单词。这也被认为是一个one-hot 编码。所以，我们可以用下面这个向量来表示单词“am”：</p>
<p><img src="https://n.sinaimg.cn/front/793/w1080h513/20190108/9pvb-hrkkwef7010082.jpg" alt="img"></p>
<p>例子：对我们输出词表的one-hot 编码</p>
<p>接下来我们讨论模型的损失函数——这是我们用来在训练过程中优化的标准。通过它可以训练得到一个结果尽量准确的模型。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>比如说我们正在训练模型，现在是第一步，一个简单的例子——把“merci”翻译为“thanks”。</p>
<p>这意味着我们想要一个表示单词“thanks”概率分布的输出。但是因为这个模型还没被训练好，所以不太可能现在就出现这个结果。</p>
<p><img src="https://jalammar.github.io/images/t/transformer_logits_output_and_label.png" alt="img"></p>
<p>因为模型的参数（权重）都被随机的生成，（未经训练的）模型产生的概率分布在每个单元格&#x2F;单词里都赋予了随机的数值。我们可以用真实的输出来比较它，然后用反向传播算法来略微调整所有模型的权重，生成更接近结果的输出。</p>
<p>如何比较两个概率分布？我们可以简单地用其中一个减去另一个。更多细节请参考交叉熵和KL散度。</p>
<p>交叉熵：<a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-09-Visual-Information/">https://colah.github.io/posts/2015-09-Visual-Information/</a></p>
<p>KL散度：<a target="_blank" rel="noopener" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained</a></p>
<p>但注意到这是一个过于简化的例子。更现实的情况是处理一个句子。例如，输入“je suis étudiant”并期望输出是“i am a student”。那我们就希望我们的模型能够成功地在这些情况下输出概率分布：</p>
<p>每个概率分布被一个以词表大小（我们的例子里是6，但现实情况通常是3000或10000）为宽度的向量所代表。</p>
<p>第一个概率分布在与“i”关联的单元格有最高的概率</p>
<p>第二个概率分布在与“am”关联的单元格有最高的概率</p>
<p>以此类推，第五个输出的分布表示“<eos>”关联的单元格有最高的概率</p>
<p>目标概率分布：</p>
<p><img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png" alt="img"></p>
<p>在一个足够大的数据集上充分训练后，我们希望模型输出的概率分布看起来像这个样子：</p>
<p><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png" alt="img"></p>
<p>注意到每个位置（词）都得到了一点概率，即使它不太可能成为那个时间步的输出——这是softmax的一个很有用的性质，它可以帮助模型训练。</p>
<p>因为这个模型一次只产生一个输出，不妨假设这个模型只选择概率最高的单词，并把剩下的词抛弃。这是其中一种方法（叫贪心解码）。另一个完成这个任务的方法是留住概率最靠高的两个单词（例如I和a），那么在下一步里，跑模型两次：其中一次假设第一个位置输出是单词“I”，而另一次假设第一个位置输出是单词“me”，并且无论哪个版本产生更少的误差，都保留概率最高的两个翻译结果。然后我们为第二和第三个位置重复这一步骤。这个方法被称作集束搜索（beam search）。在我们的例子中，集束宽度是2（因为保留了2个集束的结果，如第一和第二个位置），并且最终也返回两个集束的结果（top_beams也是2）。这些都是可以提前设定的参数。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">re-burn</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://re-burn.github.io/2022/04/12/transformer/">https://re-burn.github.io/2022/04/12/transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://re-burn.github.io">re-burn 笔记堆</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/transformer/">transformer</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/04/12/ViT/"><i class="fa fa-chevron-left">  </i><span>Vision Transformer</span></a></div><div class="next-post pull-right"><a href="/2022/04/12/I3D/"><span>Two-Stream Inflated 3D ConvNets</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2022 By re-burn</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>