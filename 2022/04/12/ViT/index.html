<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Vision Transformer"><meta name="keywords" content="计算机视觉,ViT,transformer"><meta name="author" content="re-burn"><meta name="copyright" content="re-burn"><title>Vision Transformer | re-burn 笔记堆</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.1.0'
} </script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Vision-Transformer"><span class="toc-number">1.</span> <span class="toc-text">Vision Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#arXiv-2010-11929-ViT"><span class="toc-number">1.1.</span> <span class="toc-text">arXiv:2010.11929 (ViT)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ViT%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">1.1.2.</span> <span class="toc-text">ViT的结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E9%83%A8%E5%88%86"><span class="toc-number">1.1.3.</span> <span class="toc-text">实验部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.1.4.</span> <span class="toc-text">模型可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#arXiv-2105-01601"><span class="toc-number">1.2.</span> <span class="toc-text">arXiv:2105.01601</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-1"><span class="toc-number">1.2.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mixer%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.2.</span> <span class="toc-text">Mixer结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.2.3.</span> <span class="toc-text">实验结果</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">re-burn</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">8</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">9</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">4</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">re-burn 笔记堆</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">Vision Transformer</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-04-12</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">日常学习笔记</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%97%A5%E5%B8%B8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h1><p>本文重点介绍ViT原理，同时简单介绍三篇相关论文，这四篇论文的源码见 <a href="https://link.zhihu.com/?target=https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.11929">arXiv:2010.11929</a>：An image is worth 16x16 words: Transformers for image recognition at scale（ViT大法，一般人没钱做的工作）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2105.01601">arXiv:2105.01601</a>：MLP-Mixer: An all-MLP Architecture for Vision （用MLPs替代self-attention可以得到和ViT同样好的结果）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2106.01548">arXiv:2106.01548</a>：When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations （不使用大规模预训练和强数据增强ViT是否依然可以表现优秀）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2106.10270">arXiv:2106.10270</a>：How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers （通过大量实验，总共训练了超过5w个ViT，教你如何训练自己的ViT模型，以及数据增广和模型正则化什么时候有用）</p>
<p>有关transformer结构和原理，大家可以参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410258597">Transformer解析</a></p>
<h2 id="arXiv-2010-11929-ViT"><a href="#arXiv-2010-11929-ViT" class="headerlink" title="arXiv:2010.11929 (ViT)"></a>arXiv:2010.11929 (ViT)</h2><p><img src="https://pic2.zhimg.com/80/v2-692d349ee289e1cea9d7d100b6919771_720w.jpg" alt="img"></p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h3><p>ViT是2020年Google团队提出的将Transformer应用在图像分类的模型，虽然不是第一篇将transformer应用在视觉任务的论文，但是因为其模型“简单”且效果好，可扩展性强（scalable，模型越大效果越好），成为了transformer在CV领域应用的里程碑著作，也引爆了后续相关研究</p>
<p>把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果</p>
<p>但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality&#x2F;two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变性（translation equivariance）， <img src="https://www.zhihu.com/equation?tex=f(g(x))=g(f(x))" alt="[公式]"> ，其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型</p>
<h3 id="ViT的结构"><a href="#ViT的结构" class="headerlink" title="ViT的结构"></a>ViT的结构</h3><p>ViT将输入图片分为多个patch（16x16），再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同。但是因为对图片分类，因此在输入序列中加入一个特殊的token，该token对应的输出即为最后的类别预测</p>
<p><img src="https://pic4.zhimg.com/80/v2-5afd38bd10b279f3a572b13cda399233_720w.jpg" alt="ViT只使用了Transformer的encoder"></p>
<p>按照上面的流程图，一个ViT block可以分为以下几个步骤</p>
<p>(1) patch embedding：例如输入图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，则每张图像会生成224x224&#x2F;16x16&#x3D;196个patch，即输入序列长度为<strong>196</strong>，每个patch维度16x16x3&#x3D;<strong>768</strong>，线性投射层的维度为768xN (N为token的维度，这里N&#x3D;768)，因此输入通过线性投射层之后的维度依然为196x768，即一共有196个token，每个token的维度是768。这里还需要加上一个特殊字符cls，因此最终的维度是<strong>197x768</strong>。到目前为止，已经通过patch embedding将一个视觉问题转化为了一个seq2seq问题</p>
<p>(2) positional encoding（standard learnable 1D position embeddings）：ViT同样需要加入位置编码，位置编码可以理解为一张表，表一共有N行，N的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列embedding的维度相同（768）。注意位置编码的操作是sum，而不是concat。加入位置编码信息之后，维度依然是<strong>197x768</strong></p>
<p>(3) LN&#x2F;multi-head attention&#x2F;LN：LN输出维度依然是197x768。多头自注意力时，先将输入映射到q，k，v，如果只有一个头，qkv的维度都是197x768，如果有12个头（768&#x2F;12&#x3D;64），则qkv的维度是197x64，一共有12组qkv，最后再将12组qkv的输出拼接起来，输出维度是197x768，然后在过一层LN，维度依然是<strong>197x768</strong></p>
<p>(4) MLP：将维度放大再缩小回去，197x768放大为197x3072，再缩小变为<strong>197x768</strong></p>
<p>一个block之后维度依然和输入相同，都是197x768，因此可以堆叠多个block。最后会将特殊字符cls对应的输出 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bz%7D_L%5E0"> 作为encoder的最终输出 ，代表最终的image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），如下图公式(4)，后面接一个MLP进行图片分类</p>
<p><img src="https://pic2.zhimg.com/80/v2-ebf697b1994598019a6a59855dc0dbed_720w.jpg"></p>
<p>其中输入image <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%5Cin%5Cmathbb%7BR%7D%5E%7BH%5Ctimes+W%5Ctimes+C%7D"> ，2D patches <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_p%5Cin%5Cmathbb%7BR%7D%5E%7BN%5Ctimes+(P%5E2%5Ccdot+C)%7D"> ， <img src="https://www.zhihu.com/equation?tex=C"> 是通道数， <img src="https://www.zhihu.com/equation?tex=P"> 是patch大小，一共有 <img src="https://www.zhihu.com/equation?tex=N"> 个patches， <img src="https://www.zhihu.com/equation?tex=N=HW/P%5E2"></p>
<p><strong>关于image presentation</strong></p>
<p>是否可以直接使用average pooling得到最终的image presentation，而不加特殊字符cls，通过实验表明，同样可以使用average pooling，原文ViT是为了尽可能是模型结构接近原始的Transformer，所以采用了类似于BERT的做法，加入特殊字符</p>
<p><img src="https://pic1.zhimg.com/80/v2-4a8b39b1d2dd43d1e9b16edbc38b1660_720w.jpg" alt="学习率的影响较大，注意调参"></p>
<p><strong>关于positional encoding</strong></p>
<p>1-D 位置编码：例如3x3共9个patch，patch编码为1到9</p>
<p>2-D 位置编码：patch编码为11,12,13,21,22,23,31,32,33，即同时考虑X和Y轴的信息，每个轴的编码维度是D&#x2F;2</p>
<p>实际实验结果表明，不管使用哪种位置编码方式，模型的精度都很接近，甚至不适用位置编码，模型的性能损失也没有特别大。原因可能是ViT是作用在image patch上的，而不是image pixel，对网络来说这些patch之间的相对位置信息很容易理解，所以使用什么方式的位置编码影像都不大</p>
<p><img src="https://pic1.zhimg.com/80/v2-e152c9ad22f6984912fb0652cf294018_720w.jpg"></p>
<p><strong>关于CNN+Transformer</strong></p>
<p>既然CNN具有归纳偏置的特性，Transformer又具有很强全局归纳建模能力，使用CNN+Transformer的混合模型是不是可以得到更好的效果呢？将224x224图片送入CNN得到16x16的特征图，拉成一个向量，长度为196，后续操作和ViT相同</p>
<p><strong>关于输入图片大小</strong></p>
<p>通常在一个很大的数据集上预训练ViT，然后在下游任务相对小的数据集上微调，已有研究表明在分辨率更高的图片上微调比在在分辨率更高的图片上预训练效果更好（It is often beneficial to fine-tune at higher resolution than pre-training）（参考<em>2019-NIPS-Fixing the train test resolution discrepancy</em>）</p>
<p>当输入图片分辨率发生变化，输入序列的长度也发生变化，虽然ViT可以处理任意长度的序列，但是预训练好的位置编码无法再使用（例如原来是3x3，一种9个patch，每个patch的位置编码都是有明确意义的，如果patch数量变多，位置信息就会发生变化），一种做法是使用插值算法，扩大位置编码表。但是如果序列长度变化过大，插值操作会损失模型性能，这是ViT在微调时的一种局限性</p>
<h3 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h3><p><strong>数据集</strong></p>
<p>为了探究模型的可扩展性（to explore model scalability），预训练阶段使用了ImageNet-1K（1.3million）、ImageNet-21K（14million），JFT-18K（303million）三个数据集。同时参考BiT，删除预训练数据集中和下游任务测试集中重复的数据（de-duplicate the pre-training datasets w.r.t. the test sets of the downstream）</p>
<p>下游数据集包括：ImageNet（on the original validation labels），ImageNet （on the cleaned-up ReaL labels ），CIFAR-10&#x2F;100，Oxford-IIIT Pets，Oxford Flowers-102，VTAB (19 tasks)</p>
<p>ImageNet ReaL参考<em>2020-Are we done with imagenet?</em> VTAB参考<em>2019-A large-scale study of representation learning with the visual task adaptation benchmark</em>，所有数据集的预处理参考BiT</p>
<p><strong>模型及变体</strong></p>
<p>（1）ViT：参考BERT，共设置了三种模型变体（增加了Huge变体）如下图所示。例如ViT-L&#x2F;16，代表Large变体，输入patch size为16x16。</p>
<p>（2）CNN：baseline CNNs选择ResNet，同时用Group Normalization替代Batch Normalization，使用standardized convolutions，以提升模型迁移性能。</p>
<p>（3）Hybrid：混合模型就是使用ResNet50输出的特征图，不同stage会得到不同大小的特征图，即生成不同长度序列</p>
<p><img src="https://pic2.zhimg.com/80/v2-54f717f71079becca62a0247660a171d_720w.jpg" alt="Details of Vision Transformer model variants"></p>
<p>所有模型的训练均使用Adam（ <img src="https://www.zhihu.com/equation?tex=%5Cbeta_1=0.9"> , <img src="https://www.zhihu.com/equation?tex=%5Cbeta_2=0.999"> ），batch_size设为4096，权重衰减（apply a high weight decay of 0.1），同时使用了学习率warmup策略（use a linear learning rate warmup and decay)；微调阶段，使用SGD with momentum，batch_size设为512</p>
<p><strong>实验结果</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-d5e21c0fadf2591220271021f570299d_720w.jpg" alt="img"></p>
<p><em>ViT和其它SOTA模型性能对比，展示了准确率accuraces的均值和标准差，所有结果都是取三轮微调均值的结果（averaged over three fine-tunning runs）。有关ImageNet的实验，在更高分辨率图片上微调(512 for ViT-L&#x2F;16 and 518 for ViT-H&#x2F;14)，同时使用了Polyak averaging(0.9999)</em></p>
<p>可以看到在JFT数据集上预训练的ViT模型，迁移到下游任务后，表现要好于基于ResNet的BiT和基于EfficientNet的Noisy Student，且需要更少的预训练时间</p>
<p><img src="https://pic2.zhimg.com/80/v2-ff04d612cd03446278622d368ec1c6c9_720w.jpg" alt="各类模型在VTAB上的表现，ViT同样性能更好"></p>
<p>上面的实验显示，当在很大的数据集上预训练时，ViT性能超越CNN，后面探究不同大小预训练数据集对模型性能的影响（不能只看超大数据集）</p>
<p><img src="https://pic2.zhimg.com/80/v2-65138c61d2f9c57448b4ba23dad4af55_720w.jpg" alt="*transfor to ImageNet*"></p>
<p>这里当在更小的数据集上预训练时（ImageNet），优化三个超参数以提升模型性能，分别是weight decay, dropout 和 label smoothing。可以看到当在小数据集上预训练时（ImageNet-1k，1.3million），ViT微调后的效果远远比不上ResNet；在中等数据集上预训练时（ImageNet-21K，14million），两者效果相当；当在很大的数据集上（JFT-300M, 300million）预训练时，ViT的效果要更好。所以当我们只有较小的数据集时，更适合使用ResNet（并不是所有数据集都适合硬套transformer）</p>
<p><img src="https://pic1.zhimg.com/80/v2-719f695ec8fb9c33e19fd8515c1ff230_720w.jpg" alt="Linear few-shot evaluation on ImageNet versus pre-training size"></p>
<p>如上图，在同一个数据集（JFT），分别抽取不同数量的数据（10M，30M，100M，300M），避免不同数据集之间的gap，同时不适用额外的regularization，超参数保证相同。linear evaluation是指直接把预训练模型当做特征提取器，不fine-tune，拿提取到的特征直接做logistic regression。few-shot是指在evaluation的时候，每一类只sample五张图片。</p>
<p>可以看到当数据集很小时，CNN预训练模型表现更好，证明了CNN归纳偏置的有效性，但是当数据集足够大时，归纳偏置和Transformer比较就失去了优势，甚至没有归纳偏置，直接从数据learn patterns会更有效。同时细心观察会发现即使预训练的数据集很大，最后ViT的性能提升也不是很明显，因此如何使用ViT来做这种小样本学习任务，是一个有待继续研究的方向</p>
<p><img src="https://pic4.zhimg.com/80/v2-53da3593bafc05bad3b72099583d909b_720w.jpg" alt="img"><em>Performance versus cost for different architectures: Vision Transformers, ResNets, andhybrids.</em></p>
<p>上图实验证明了ViT的预训练比ResNet要更便宜，即在相同的预训练计算复杂度下，ViT的效果要比ResNet更好。可以看到，当模型较小时，混合模型的表现要更好，但是随着模型的增大，ViT的表现超过了混合模型（为什么混合模型这个时候不如ViT，直觉上混合模型吸收了双方的优点，应该表现更好）。</p>
<h3 id="模型可视化"><a href="#模型可视化" class="headerlink" title="模型可视化"></a>模型可视化</h3><p><img src="https://pic2.zhimg.com/80/v2-fb19c6d629b419b02f26b4db31598a51_720w.jpg" alt="img"></p>
<p><em>ViT block第一层（linear projection)的前28个主成分</em></p>
<p><img src="https://pic2.zhimg.com/80/v2-99f02198921e7aed8162cd7af8a29805_720w.jpg" alt="img"></p>
<p><em>位置编码得相似性分析(cos)，位置越接接近，patches之间的相似度越高；相同行&#x2F;列的patches有相似的embeddings；</em></p>
<p>为了理解self-attention是如何聚合信息的（To understand how ViT uses self-attention to integrate information across the image），基于attention weight计算不同layer不同head的average attention distance</p>
<p><img src="https://pic4.zhimg.com/80/v2-8539fe277eae097183add2d6d2f559e3_720w.jpg" alt="img"></p>
<p><em>每一个layer的每一个head的average attention distance，类似于CNN感受野的概念，可以发现一些head在第一个layer就attent到了几乎整张图片的范围</em></p>
<p>average attention distance，是基于attention weight计算，具体做法是用attention weight乘以query pixel和所有其它pixels的距离，再求平均。原文中是这么表述的——Attention distance was computed for 128 example images by averaging the distance between the query pixel and all other pixels, weighted by the attention weight. Each dot shows the mean attention distance across images for one of 16 heads at one layer. Image width is 224 pixels.</p>
<p><img src="https://pic4.zhimg.com/80/v2-dabb6afe03b02498e0a8081d00c0b437_720w.jpg" alt="img"></p>
<p><em>Representative examples of attention from the output token to the input space.</em></p>
<h2 id="arXiv-2105-01601"><a href="#arXiv-2105-01601" class="headerlink" title="arXiv:2105.01601"></a>arXiv:2105.01601</h2><p><img src="https://pic3.zhimg.com/80/v2-30735a1641e51d73675e69a99dfe0476_720w.jpg" alt="img"></p>
<h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>ViT作者团队出品，在CNN和Transformer大火的背景下，舍弃了卷积和注意力机制，提出了MLP-Mixer，一个完全基于MLPs的结构，其MLPs有两种类型，分别是<strong>channel-mixing MLPs</strong>和<strong>token-mixing MLPs</strong>，前者独立作用于image patches（融合通道信息），后者跨image patches作用（融合空间信息）。实验结果表明该结构和SOTA方法同样出色，证明了convolution和attention不是必要操作，如果将其替换为简单的MLP，模型依然可以完美work</p>
<blockquote>
<p>MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. “mixing” the per-location features), and one with MLPs applied across patches (i.e. “mixing” spatial information) &#x2F; MLP-Mixer’s architecture is based entirely on multi-layer perceptrons (MLPs) that are repeatedly applied across either spatial locations or feature channels &#x2F; Mixer makes use of two types of MLP layers: channel-mixing MLPs and token-mixing MLPs</p>
</blockquote>
<h3 id="Mixer结构"><a href="#Mixer结构" class="headerlink" title="Mixer结构"></a>Mixer结构</h3><p><img src="https://pic3.zhimg.com/80/v2-ed96c7a5add85b9151c7f10fbda4943a_720w.jpg" alt="img"></p>
<p><em>类似于Transformer，每一个layer输入和输出的维度相同，可以堆叠多个layer</em></p>
<p>类似于ViT，首先进行patch embedding操作，<strong>一个Mixer Layer中包含了channel-mixing MLPs和token-mixing MLPs</strong>，但是Mixer不适用positional encoding，因为token-mixing MLPs对输入tokens的顺序非常敏感</p>
<p><img src="https://pic2.zhimg.com/80/v2-58df6c737fdea377071868f106b7c1c1_720w.jpg"></p>
<p>token-mixing MLPs：允许信息在空间维度交互，独立作用于每一个channel，作用于列，融合不同token的特征</p>
<p>channel-mixing MLPs：允许信息在通道交互，独立作用于每一个token，作用于行，融合不同channel的特征</p>
<p>输入image的分辨率为 <img src="https://www.zhihu.com/equation?tex=H%5Ctimes+W"> ，patch的分辨率为 <img src="https://www.zhihu.com/equation?tex=P%5Ctimes+P">，则patch的数量 <img src="https://www.zhihu.com/equation?tex=S=HW/P%5E2"> ，所有的patch拉直后线性投影到维度 <img src="https://www.zhihu.com/equation?tex=C"> ，则得到Mixer Layer的输入 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D%5Cin%5Cmathbb%7BR%7D%5E%7BS%5Ctimes+C%7D"> 。token-mixing MLPs作用于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D"> 的列，特征维度不发生变化（ <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5ES%5Crightarrow%5Cmathbb%7BR%7D%5ES"> ），channel-mixing MLPs作用于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D"> 的行，特征维度同样不发生变化（ <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5EC%5Crightarrow%5Cmathbb%7BR%7D%5EC"> ）。每一个MLP包含两个全连接层和一个非线性激活（GELU），一个Mixer layers的公式如下，计算复杂度和输入patches的数量成线性关系（ViT是平方关系)</p>
<p><img src="https://pic3.zhimg.com/80/v2-a0abfc39a823b43e061ee5acd14da0e2_720w.jpg"></p>
<p>需要注意token-mixing MLPs共享参数，channel-mixing MLPs同样共享参数，因此避免了当输入特征维度增加（ <img src="https://www.zhihu.com/equation?tex=C"> 变大）或者输入序列长度增加（ <img src="https://www.zhihu.com/equation?tex=S"> 变大)时，模型参数量急剧增加的情况，极大减少了内存消耗</p>
<p>模型最后接global average pooling+a linear classifier</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>当在大规模数据集上预训练（100million images），Mixer可以接近CNNs和Transformers的SOTA表现，在ImageNet上达到87.94%的top-1 accuracy；当在更小规模数据集上预训练时（10million），结合一些regularization techniques，Mixer可以接近ViT的性能，但是稍逊于CNN</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">re-burn</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://re-burn.github.io/2022/04/12/ViT/">https://re-burn.github.io/2022/04/12/ViT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://re-burn.github.io">re-burn 笔记堆</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><a class="post-meta__tags" href="/tags/ViT/">ViT</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/04/12/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"><i class="fa fa-chevron-left">  </i><span>知识蒸馏</span></a></div><div class="next-post pull-right"><a href="/2022/04/12/transformer/"><span>transformer</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2022 By re-burn</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>